Big O Notation is a way to formalize fuzzy counting

It allows us to talk formally about how the runtime of an algorithm grows as the inputs grow

We won't care about the details, only the trends

We say that an algorithm is **O(f(n))** if the number of simple operations the computer has to do is eventually less than a constant times **f(n)**, as **n** increases

-   f(n) could be linear (f(n) = n)
-   f(n) could be quadratic (f(n) = n  )
-   f(n) could be constant (f(n) = 1)
-   f(n) could be something entirely different!